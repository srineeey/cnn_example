{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "\n",
    "import PIL\n",
    "\n",
    "import dataset_class_image\n",
    "import torch_net_class\n",
    "\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Utilizing CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizing CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function for training loop\n",
    "\"\"\"\n",
    "\n",
    "def step(model, input_data, batch_size, loss_func, optimizer, epoch, batch_nr, device, log_file, mode=\"val\"):\n",
    "        if mode == \"train\":\n",
    "            model.train()\n",
    "        elif mode == \"val\":\n",
    "            model.eval()\n",
    "        \n",
    "        #input data splitted into features and labels\n",
    "        feat_batch = input_data[0].to(device)\n",
    "        label_batch = input_data[1].to(device)\n",
    "        \n",
    "        #input_size = model.get_input_size()\n",
    "        #output_size = model.get_output_size()\n",
    "    \n",
    "        \"\"\"ALWAYS SET GRADIENT TO ZERO  FOR STANDARD NN (NOT RNNs)\"\"\"\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        the peculiar shape (-1, sample_size) is needed, because an entire mini batch is passed on to the network\n",
    "        initially it is not clear how large such a mini batch is\n",
    "        the -1 acts as a placeholder in order to keep the number of processed items in one mini batch flexible\n",
    "        \"\"\"\n",
    "        #print(\"Input {}, batch: {} : {}\".format(epoch, batch_nr, feat_batch))\n",
    "        #log_file.write(\"Input {}, batch: {} : {}\\n\".format(epoch, batch_nr, feat_batch))\n",
    "        #plt.imshow( input_data[0].view((28,28)) )\n",
    "        \n",
    "        model_input_shape = tuple([-1] + list(input_size))\n",
    "        #print(\"Reshaping to {}\".format( feat_batch.view(model_input_shape).double().size() ))\n",
    "        output = model(feat_batch.view(model_input_shape).float())\n",
    "        output = output.to(device)\n",
    "    \n",
    "        #print(\"Output {}, batch: {} : {}\".format(epoch, batch_nr, output))\n",
    "        #print(\"Label {}, batch: {} : {}\".format(epoch, batch_nr, label_batch))\n",
    "        #log_file.write(\"Output {}, batch: {} : {}\\n\".format(epoch, batch_nr, output))\n",
    "        \n",
    "        #print(\"Input shape {}\".format(feat_batch.size()))\n",
    "        #print(\"Label shape {}\".format(label_batch.size()))\n",
    "        #print(\"Output shape {}\".format(output.size()))\n",
    "    \n",
    "        #print(\"Feeding forward epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Feeding forward epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "    \n",
    "        #print(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "        model_output_shape = tuple([-1,output_size])\n",
    "        \"\"\"MSE\"\"\"\n",
    "        #loss = loss_func(output.view(-1, output_size).float(), label_batch.view(-1, output_size).float())\n",
    "        \"\"\"Cross Entropy\"\"\"\n",
    "        loss = loss_func(output.view(model_output_shape).float(), label_batch.view(-1).long())\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            #print(\"epoch: {}, batch: {}, loss: {}\".format(epoch, batch_nr, loss.item()))\n",
    "            #print(\"Performing backprop ...\")\n",
    "            #log_file.write(\"epoch: {}, batch: {}, loss: {}\\n\".format(epoch, batch_nr, loss.item()))\n",
    "            #log_file.write(\"Performing backprop ...\\n\")\n",
    "            loss.backward()\n",
    "    \n",
    "            #print(\"Adjusting weights ...\")\n",
    "            #log_file.write(\"Adjusting weights ...\\n\")\n",
    "            optimizer.step()\n",
    "    \n",
    "        return loss, output\n",
    "\n",
    "\"\"\"\n",
    "function for calculating neuron activation layer sizes.\n",
    "needed for torch_net_class (has been copied there)\n",
    "\"\"\"\n",
    "    \n",
    "def calc_layer_sizes(input_shape, net_struct):\n",
    "    layer_sizes = [input_shape]\n",
    "    \n",
    "    for i in range(len(net_struct)):\n",
    "        \n",
    "        new_layer_size = []\n",
    "        if net_struct[i][\"type\"] == nn.Linear:\n",
    "            #print(net_struct[i][\"type\"])\n",
    "            new_layer_size = net_struct[i][\"layer_pars\"][\"out_features\"]\n",
    "            \n",
    "        elif net_struct[i][\"type\"] == nn.Conv2d:\n",
    "            kernel_shape = net_struct[i][\"layer_pars\"][\"kernel_size\"]\n",
    "            stride = net_struct[i][\"layer_pars\"][\"stride\"]\n",
    "            \n",
    "            new_layer_size = []\n",
    "            #print(\"layer \" + str(i))\n",
    "            for d in range(len(kernel_shape)):\n",
    "                prev_layer_l = int(layer_sizes[-1][d+1])\n",
    "                kernel_l = int(kernel_shape[d])\n",
    "                new_layer_size.append( (prev_layer_l - kernel_l)//stride + 1 )\n",
    "            new_layer_size = [net_struct[i][\"layer_pars\"][\"out_channels\"]] + new_layer_size\n",
    "              \n",
    "        elif net_struct[i][\"type\"] == nn.MaxPool2d:\n",
    "            kernel_shape = net_struct[i][\"layer_pars\"][\"kernel_size\"]\n",
    "            stride = net_struct[i][\"layer_pars\"][\"stride\"]\n",
    "            \n",
    "            new_layer_size = []\n",
    "            for d in range(len(kernel_shape)):\n",
    "                prev_layer_l = int(layer_sizes[-1][d+1])\n",
    "                kernel_l = int(kernel_shape[d])\n",
    "                new_layer_size.append( (prev_layer_l - kernel_l)//stride + 1 )\n",
    "            \n",
    "            #new_layer_size = [(layer_sizes[-1][d+1] - kernel_shape[d])/stride + 1 for d in range(len(kernel_shape))]\n",
    "            \n",
    "            prev_channels = layer_sizes[-1][0]\n",
    "            new_layer_size = [prev_channels] + new_layer_size\n",
    "        \n",
    "        elif net_struct[i][\"type\"] == nn.BatchNorm1d or net_struct[i][\"type\"] == nn.Dropout or net_struct[i][\"type\"] == nn.Softmax:\n",
    "                new_layer_size = layer_sizes[-1]\n",
    "        \n",
    "        layer_sizes.append(new_layer_size)\n",
    "    \n",
    "    return layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '../GermanTrafficSignDatasetWithBackdoorImages/CleanDataset/train'\n",
    "image_names = os.listdir(image_folder)\n",
    "image_paths = glob.glob(image_folder + \"/*/*.ppm\")\n",
    "#print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GermanTrafficSignDatasetWithBackdoorImages/CleanDataset/train/Pedestrian/G_GTSDB_487.ppm\n",
      "Pedestrian\n",
      "(21, 24)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "with PIL.Image.open(image_paths[4]) as test_image:\n",
    "    print(image_paths[4])\n",
    "    print(image_paths[4].split(\"/\")[-2])\n",
    "    print(test_image.size)\n",
    "    plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_res = 32\n",
    "\n",
    "custom_transforms = [transforms.RandomResizedCrop(image_res, scale=(0.8,1.0)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])]\n",
    "#custom_transforms = [transforms.RandomResizedCrop(image_res), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])]\n",
    "\n",
    "#custom_transforms = [transforms.Resize(image_res), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])]\n",
    "\n",
    "\n",
    "custom_transforms = transforms.Compose(custom_transforms)\n",
    "dataset = dataset_class_image.image_dataset(image_folder, image_paths, image_res=image_res, transform=custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 2 2 2]\n",
      "4757\n"
     ]
    }
   ],
   "source": [
    "print(dataset.labels)\n",
    "print(len(dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedestrian\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load exempalr image\"\"\"\n",
    "test_example = dataset.get_image(0)\n",
    "print(dataset.label_encoder.classes_[test_example[1]])\n",
    "print(test_example[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"display the (augmented) image\"\"\"\n",
    "example_image = test_example[0].detach().cpu().numpy()\n",
    "plt.imshow(example_image[0])\n",
    "print(example_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 32, 32], [18, 28, 28], [18, 14, 14], [36, 10, 10], [36, 5, 5], 900, 900, 900, 900, 8, 8]\n",
      "size of train set :4281\n",
      "one batch with size:6\n",
      "size of val set :475\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "#val_epochs = [2,16,32,48,60]\n",
    "val_epochs = [25,50,75,100]\n",
    "#val_epochs = [1]\n",
    "save_state_epochs = [102]\n",
    "\n",
    "\n",
    "\"\"\"Manually set network structure\"\"\"\n",
    "\"\"\"\n",
    "    This list can be loaded into the constructor of the Net neural network class, to automatically generate the network structure\n",
    "    type = pointer to the layer function'\n",
    "    layer_pars = parameters which must be given to the layer function in order to initialize it\n",
    "    act_func = activation function to be applied directly after feeding to the corresponding layer\n",
    "    dropout = certain neurons cna be dropped out if specified\n",
    "\"\"\"\n",
    "\n",
    "fixed_net_struct = []\n",
    "image_size = dataset.get_image_size()\n",
    "input_size = image_size\n",
    "#target_size = 1\n",
    "output_size = len(dataset.label_encoder.classes_)\n",
    "\n",
    "#[ [[in_channels, out_channels],[kernel_size], ...]\n",
    "#kernel_pars = [ [[input_size[0],33],[3,3]], [[33,33],[2,2]], [[33,66],[3,3]], [[66,66],[2,2]], [[66,66],[3,3]] ]\n",
    "#kernel_pars = [ [[input_size[0],33],[9,9]], [[33,33],[4,4]], [[33,66],[6,6]], [[66,66],[5,5]], [[66,66],[5,5]] ]\n",
    "#kernel_pars = [ [[input_size[0],48],[11,11]], [[48,48],[3,3]], [[48,96],[5,5]], [[96,96],[3,3]], [[96,192],[3,3]], [[192,192],[2,2]] ]\n",
    "kernel_pars = [ [[input_size[0],18],[5,5]], [[18,18],[2,2]], [[18,36],[5,5]], [[36,36],[2,2]]]\n",
    "act_func = torch.relu\n",
    "\n",
    "for i, kernel_par in enumerate(kernel_pars):\n",
    "    if i%2 == 0:\n",
    "        layer_type = nn.Conv2d\n",
    "        fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"in_channels\": kernel_par[0][0], \"out_channels\": kernel_par[0][1], \"kernel_size\": kernel_par[1], \"stride\": 1,  \"bias\": True}} )\n",
    "    else:\n",
    "        layer_type = nn.MaxPool2d\n",
    "        fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"kernel_size\": kernel_par[1], \"stride\": kernel_par[1][1]}} )\n",
    "    \n",
    "    #fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"in_channels\": kernel_par[0][0], \"out_channels\": kernel_par[0][1], \"kernel_size\": kernel_par[1], \"bias\": True}} )\n",
    "\n",
    "conv_sizes = calc_layer_sizes(input_size, fixed_net_struct)\n",
    "fc_input_size = np.product(conv_sizes[-1])\n",
    "    \n",
    "fixed_net_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size, \"out_features\": 900}, \"bias\": True, \"act_func\": act_func} )\n",
    "fixed_net_struct.append( {\"type\": nn.BatchNorm1d, \"layer_pars\": {\"num_features\": 900}} )\n",
    "fixed_net_struct.append( {\"type\": nn.Dropout, \"layer_pars\": {\"p\": 0.2 }} )\n",
    "#fixed_net_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": 594, \"out_features\": 594}, \"bias\": True, \"act_func\": act_func} )\n",
    "fixed_net_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": 900, \"out_features\": 900}, \"bias\": True, \"act_func\": act_func} )\n",
    "fixed_net_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": 900, \"out_features\": output_size}, \"bias\": True, \"act_func\": act_func} )\n",
    "fixed_net_struct.append( {\"type\": nn.Softmax, \"layer_pars\": {\"dim\": 0}} )\n",
    "\n",
    "conv_sizes = calc_layer_sizes(input_size, fixed_net_struct)\n",
    "print(conv_sizes)\n",
    "\n",
    "#print(fixed_net_struct)\n",
    "\n",
    "\"\"\"If required create list of parameters manually\"\"\"\n",
    "\n",
    "parameter_option = {}\n",
    "parameter_option[\"task\"] = \"classification\"\n",
    "parameter_option[\"loss_func\"] = nn.CrossEntropyLoss\n",
    "parameter_option[\"optimizer\"] = optim.Adam\n",
    "parameter_option[\"batch_size\"] = 9\n",
    "parameter_option[\"lr\"] = 0.00001\n",
    "parameter_option[\"net_struct\"] = fixed_net_struct\n",
    "\n",
    "parameter_option[\"val\"] = \"holdout\"\n",
    "parameter_option[\"split\"] = {\"train\" : 0.9, \"val\" : 0.1, \"test\" : 0.0}\n",
    "#parameter_option[\"val\"] = \"k_fold\"\n",
    "#parameter_option[\"ksplits\"] = 5\n",
    "\n",
    "hyper_parameters = parameter_option\n",
    "\n",
    "print(\"size of train set :{}\".format(int(parameter_option[\"split\"][\"train\"]*dataset.get_length())))\n",
    "print(\"one batch with size:{}\".format(int(parameter_option[\"split\"][\"train\"]*dataset.get_length())%parameter_option[\"batch_size\"]))\n",
    "print(\"size of val set :{}\".format(int(parameter_option[\"split\"][\"val\"]*dataset.get_length())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "mean epoch 0 loss: 1.374305268045233\n",
      "mean epoch 1 loss: 1.2837250255236106\n",
      "mean epoch 2 loss: 1.2805016378895575\n",
      "mean epoch 3 loss: 1.2771844673557442\n",
      "mean epoch 4 loss: 1.2763657614964397\n",
      "mean epoch 5 loss: 1.2762678628709136\n",
      "mean epoch 6 loss: 1.27759213758116\n",
      "mean epoch 7 loss: 1.278839597932431\n",
      "mean epoch 8 loss: 1.2754125449837757\n",
      "mean epoch 9 loss: 1.2742796740612061\n",
      "mean epoch 10 loss: 1.27432951806974\n",
      "mean epoch 11 loss: 1.2755384835876336\n",
      "mean epoch 12 loss: 1.274353974256195\n",
      "mean epoch 13 loss: 1.2740315061156489\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All errors from each validation epoch will be saved\n",
    "this list contains them\n",
    "Saving them in memory is not always viable, which is why they are also saved on the hard drive\n",
    "\"\"\"\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label \n",
    "model_errors = []\n",
    "\"\"\"\n",
    "All errors from each validation epoch will be saved as a file\n",
    "this list contains the path to these files\n",
    "\"\"\"\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label \n",
    "model_errors_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "The average training loss for each epoch is recorded here\n",
    "\"\"\"\n",
    "#second is fold\n",
    "loss_curves = []\n",
    "loss_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "States of the neural network are saved as a file\n",
    "filename is part of this list\n",
    "\"\"\"\n",
    "#second is fold\n",
    "#third is epoch\n",
    "saved_states_file_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "Set/copy the hyper_parameters\n",
    "\"\"\"\n",
    "epochs = max(val_epochs)\n",
    "#input_size = len(pred_attributes)\n",
    "#target_size = len(target_attributes)\n",
    "#output_size = len(output_attributes)\n",
    "\n",
    "lr=hyper_parameters[\"lr\"]\n",
    "batch_size = hyper_parameters[\"batch_size\"]\n",
    "loss_func = hyper_parameters[\"loss_func\"]()\n",
    "\n",
    "net_struct = hyper_parameters[\"net_struct\"]\n",
    "\n",
    "\"\"\"Choose between holdout validation and k-fold cross validation\"\"\"\n",
    "if hyper_parameters[\"val\"] == \"holdout\":\n",
    "    all_indices = list(range(dataset.get_length()))\n",
    "    indices = [None, None]\n",
    "    split_ratio = hyper_parameters[\"split\"]\n",
    "    indices[0], indices[1] = train_test_split(all_indices, test_size=split_ratio[\"val\"], shuffle=True, random_state=random_seed)\n",
    "    indices = [indices]\n",
    "elif hyper_parameters[\"val\"] == \"k_fold\":\n",
    "    \"\"\"NOT WORKING ATM\"\"\"\n",
    "    n_splits = hyper_parameters[\"k_splits\"]\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    indices = kf.split(dataset.df)\n",
    "\n",
    "    #if dataset is split into train test and val, use only train and val for KFold\n",
    "    #split_indices[\"train_val\"] = split_indices[\"train\"] + split_indices[\"val\"]\n",
    "    #indices = kf.split(dataset.df.iloc[split_indices[\"train_val\"]])\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\n",
    "fold_model_errors = []\n",
    "fold_model_errors_path_list = []\n",
    "fold_saved_states_path_list = []\n",
    "\n",
    "n_split = 0\n",
    "\"\"\"iterate through all folds (1 for holdout, k for kfold)\"\"\"\n",
    "for fold_indices in indices:\n",
    "\n",
    "    train_indices = fold_indices[0]\n",
    "    val_indices = fold_indices[1]\n",
    "\n",
    "    fold_loss_curve = []\n",
    "    print(\"fold {}\".format(n_split))\n",
    "    fold_dir = \"fold_{}\".format(str(n_split))\n",
    "    try:\n",
    "        os.makedirs(fold_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    \"\"\"TRANSFER LEARNING TRIAL START\"\"\"\n",
    "    \n",
    "    custom_net = models.vgg16(pretrained=True)\n",
    "    for param in custom_net.parameters():\n",
    "        param.requires_autograd = False\n",
    "    n_inputs = 4096\n",
    "    my_classifier = nn.Sequential(\n",
    "                      nn.Linear(n_inputs, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.1),\n",
    "                      nn.Linear(256, output_size),                   \n",
    "                      nn.Softmax(dim=1))\n",
    "    \n",
    "    custom_net.classifier[6] = my_classifier\n",
    "    \n",
    "    net = custom_net\n",
    "    \n",
    "    \"\"\"TRANSFER LEARNING TRIAL STOP\"\"\"\n",
    "    \"\"\"\n",
    "    #initialize network and move to GPU\n",
    "    \n",
    "    net = torch_net_class.Net(net_struct, dataset.get_image_size())\n",
    "    \n",
    "    net.init_weights(torch.nn.init.xavier_normal_)\n",
    "    net.set_batch_size(batch_size)\n",
    "    #net.cuda()\n",
    "    net.show_layers()\n",
    "    #train_log_file.write(str(net.get_net_struct()))\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "    net_parameters = net.parameters()\n",
    "    optimizer = hyper_parameters[\"optimizer\"](net_parameters, lr=lr)\n",
    "\n",
    "    #create training log\n",
    "    train_log_file_name = fold_dir +\"/train_log.txt\"\n",
    "    train_log_file = open(train_log_file_name, \"w\")\n",
    "    #train_log_file.write( \"Training log fold {} :\\n\".format(str(n_split)) )\n",
    "    \n",
    "    \n",
    "    \"\"\"split training in train and val and load data\"\"\"\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "    #test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    #val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "    train_state_dir = fold_dir + \"/net_states\"\n",
    "    try:\n",
    "        os.makedirs(train_state_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    train_loss_curve = []\n",
    "    val_loss_curve = []\n",
    "\n",
    "    fold_epoch_model_errors = []\n",
    "    fold_epoch_model_errors_path_list = []\n",
    "    fold_epoch_saved_states_path_list = []\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        batch_nr = 0\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        \"\"\"Actual training step\"\"\"\n",
    "        for train_mini_batch in train_loader:\n",
    "            batch_nr += 1\n",
    "            batch_loss, train_output = step(net, train_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"train\")\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "        \"\"\"Averaging training loss\"\"\"\n",
    "        epoch_loss = epoch_loss/len(train_loader)\n",
    "        train_loss_curve.append(epoch_loss)\n",
    "        fold_loss_curve.append(epoch_loss)\n",
    "\n",
    "        print(\"mean epoch {} loss: {}\".format(epoch, epoch_loss))\n",
    "        #train_log_file.write(\"mean epoch loss: {}\\n\".format(epoch_loss))\n",
    "\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        \"\"\"save the neural networks state\"\"\"\n",
    "        if (epoch+1) in save_state_epochs or epoch == epochs:\n",
    "            train_state_epoch_file_name = \"state_epoch_{}\".format(epoch)\n",
    "            train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(train_state, train_state_dir + \"/\" + train_state_epoch_file_name )\n",
    "            fold_epoch_saved_states_path_list.append(train_state_dir + \"/\" + train_state_epoch_file_name)\n",
    "            print(\"saved model from epoch {}\".format(epoch))\n",
    "            train_log_file.write(\"saved model from epoch {}\\n\".format(epoch))\n",
    "\n",
    "        \"\"\"\n",
    "        Valdation\n",
    "        \"\"\"\n",
    "        #validate for each epoch reached in val_epochs\n",
    "        if (epoch+1) in val_epochs:\n",
    "        #if (epoch) in val_epochs:\n",
    "\n",
    "            val_loss = []\n",
    "            val_pred = []\n",
    "            val_label = []\n",
    "            val_i = 0\n",
    "            for val_mini_batch in val_loader:\n",
    "\n",
    "                feat_batch = val_mini_batch[0]\n",
    "                label_batch = val_mini_batch[1]\n",
    "                val_label.append(label_batch.detach().cpu().numpy())\n",
    "                val_batch_loss, val_output = step(net, val_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"val\")\n",
    "\n",
    "                if hyper_parameters[\"task\"] == \"classification\":\n",
    "                    \"\"\"for a classification task\"\"\"\n",
    "                    class_batch_pred = []\n",
    "                    for val in val_output:\n",
    "                        class_index = val.argmax().detach().cpu()\n",
    "                        class_batch_pred.append(class_index)\n",
    "                    val_pred.append(class_batch_pred)\n",
    "                elif hyper_parameters[\"task\"] == \"regression\":\n",
    "                    \"\"\"for a regression task\"\"\"\n",
    "                    val_pred.append(val_output.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                val_loss.append(val_batch_loss.item())\n",
    "                #print(val_batch_loss.item())\n",
    "                val_i += 1\n",
    "\n",
    "            \"\"\"SAVE NUMPY ARRAY INSTEAD OF DF\"\"\"\n",
    "            #val_pred_df = pd.DataFrame(np.hstack(val_pred))\n",
    "            #val_label_df = pd.DataFrame(np.hstack(val_label))\n",
    "            \n",
    "            val_pred_df = pd.DataFrame(functools.reduce(operator.iconcat, val_pred, []))\n",
    "            val_label_df = pd.DataFrame(functools.reduce(operator.iconcat, val_label, []))\n",
    "\n",
    "\n",
    "            val_error_df = pd.DataFrame()\n",
    "\n",
    "            val_error_df[\"train_label\"] = val_label_df[0]\n",
    "            val_error_df[\"train_prediction\"] = val_pred_df[0]\n",
    "\n",
    "\n",
    "            fold_epoch_model_errors.append(val_error_df.copy())\n",
    "            fold_epoch_model_pred_path = fold_dir + \"/\" +\"val_epoch_{}_pred\".format(epoch)\n",
    "            fold_epoch_model_labels_path = fold_dir + \"/\" +\"val_epoch_{}_labels\".format(epoch)\n",
    "            val_pred_df.to_pickle(fold_epoch_model_pred_path)\n",
    "            val_label_df.to_pickle(fold_epoch_model_labels_path)\n",
    "            fold_epoch_model_errors_path_list.append([fold_epoch_model_pred_path, fold_epoch_model_labels_path])\n",
    "    \n",
    "    \"\"\"With this line the pred and labels for all validation epochs cna be saved -> space consuming!!!\"\"\"\n",
    "    \"\"\"fold_model_errors.append(fold_epoch_model_errors.copy())\"\"\"\n",
    "    fold_model_errors_path_list.append(fold_epoch_model_errors_path_list)\n",
    "    fold_saved_states_path_list.append(fold_epoch_saved_states_path_list)\n",
    "    loss_curves.append(fold_loss_curve)\n",
    "\n",
    "    n_split += 1\n",
    "\n",
    "    del net\n",
    "    del optimizer\n",
    "    \n",
    "\"\"\"With this line the pred and labels for all validation epochs cna be saved -> space consuming!!!\"\"\"\n",
    "\"\"\"model_errors.append(fold_model_errors.copy())\"\"\"\n",
    "model_errors_path_list.append(fold_model_errors_path_list)\n",
    "saved_states_file_path_list.append(fold_saved_states_path_list)\n",
    "#loss_curves.append(par_loss_curves)\n",
    "\n",
    "\"\"\"Plot training loss curve and save as image\"\"\"\n",
    "train_loss_img_file_name = \"train_loss.png\"\n",
    "x_epochs = range(epochs)\n",
    "for fold_i in range(len(loss_curves)):\n",
    "    #plt.plot(x_epochs, loss_curves[par_i][fold_i])\n",
    "    plt.plot(x_epochs, loss_curves[fold_i])\n",
    "#plt.title()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.savefig(train_loss_img_file_name)\n",
    "plt.close()\n",
    "train_loss_txt_file_name = \"train_loss.txt\"\n",
    "np.savetxt(train_loss_txt_file_name, loss_curves)\n",
    "\n",
    "train_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#model_errors[0][0][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label\n",
    "test_pickle_pred_df = pd.read_pickle(model_errors_path_list[0][0][-1][0])\n",
    "test_pickle_label_df = pd.read_pickle(model_errors_path_list[0][0][-1][1])\n",
    "test_pickle_label_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pickle_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pickle_label_df.columns = [\"train_label\"]\n",
    "test_pickle_pred_classes = test_pickle_pred_df.max(axis=1)\n",
    "test_pickle_pred_df[\"train_prediction\"] = pd.DataFrame(test_pickle_pred_classes)\n",
    "test_pickle_errors_df = pd.DataFrame()\n",
    "test_pickle_errors_df[\"train_prediction\"] = test_pickle_pred_df[\"train_prediction\"]\n",
    "test_pickle_errors_df[\"train_label\"] = test_pickle_label_df[\"train_label\"]\n",
    "\n",
    "test_pickle_errors_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For classification\n",
    "plot the confusion matrix to evaluate model\n",
    "\"\"\"\n",
    "conf_mat = pd.crosstab(test_pickle_errors_df[\"train_label\"], test_pickle_errors_df[\"train_prediction\"], rownames=[\"Label\"], colnames=[\"Predicted\"], margins=True)\n",
    "print(conf_mat)\n",
    "np_conf_mat = conf_mat.values.copy()\n",
    "#print(np_conf_mat)\n",
    "conf_mat_shape = list(np_conf_mat.shape)\n",
    "\n",
    "pad_extend = ((0,0),(0,output_size-conf_mat_shape[1]+1))\n",
    "#print(pad_extend)\n",
    "#print(np_conf_mat.shape)\n",
    "diag_indices = [i for i in range(0,output_size)]\n",
    "diag_indices = [diag_indices, diag_indices]\n",
    "np_conf_mat = np.pad(np_conf_mat, pad_extend, mode=\"constant\", constant_values=0)\n",
    "np_conf_mat[:,-1] = np_conf_mat[:,conf_mat_shape[1]-1]\n",
    "np_conf_mat[:,conf_mat_shape[1]-1] = 0.\n",
    "#print(np_conf_mat)\n",
    "print(np_conf_mat[diag_indices].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
